{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d3e596",
   "metadata": {},
   "source": [
    "# PS2-2 Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c019dc3",
   "metadata": {},
   "source": [
    "### (a) Prove logistic regression is well-calibrated over the range $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c83072",
   "metadata": {},
   "source": [
    "Since sigmoid function is within the range $(0, 1)$, $I_{0,1}={1,2,\\dots, m}$.\n",
    "\n",
    "By performing standard MLE, it's easy to obtain the log-likelihood $l(\\theta)$ and its derivative w.r.t. $\\theta_j$.\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j}l(\\theta)=\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)},j=0,1,\\dots,n-1.$$\n",
    "\n",
    "Ideally, when the log-likelihood is maximized, $\\forall j=0,1,\\dots,n-1, \\frac{\\partial}{\\partial \\theta_j}l(\\theta)=0$. Then from the bias term $x_0^{(i)}=1$ for all $i$ we can obtain\n",
    "that\n",
    "\n",
    "$$\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))=0$$\n",
    "\n",
    "when the log-likelihood is maximized.\n",
    "\n",
    "The equation above can be transformed to the form to be proved:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))=0\\\\\n",
    "\\Longleftrightarrow&\\sum_{i=1}^my^{(i)} = \\sum_{i=1}^mh_\\theta(x^{(i)})\\\\\n",
    "\\Longleftrightarrow&\\sum_{i\\in I_{a,b}}y^{(i)} = \\sum_{i\\in I_{a,b}}P\\left(y^{(i)}=1|x^{(i)};\\theta\\right)\\\\\n",
    "\\Longleftrightarrow&\\sum_{i\\in I_{a,b}}P\\left(y^{(i)}=1|x^{(i)};\\theta\\right)=\\sum_{i\\in I_{a,b}}1_{\\{y^{(i)}=1\\}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22356d5c",
   "metadata": {},
   "source": [
    "### (b) Relation between calibration and classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e7f23",
   "metadata": {},
   "source": [
    "##### 1.A binary classification model is perfectly calibrated does not imply that it achieves perfect accuracy.\n",
    "\n",
    "Consider a dataset containing two samples $x^{(1)}, x^{(2)}$, and $y^{(1)}=0$, $y^{(2)}=1$. To meet the perfect-calibrated condition, the model outputs must be $P(y^{(1)}=1|x^{(1)})=P(y^{(2)}=1|x^{(2)})=0.5$. But the classification accuracy is only 50%.\n",
    "\n",
    "##### 2.The converse is also not neccessarily true.\n",
    "\n",
    "Perfect accuracy can be achieved without maximizing likelihood strictly (known from PS2-1, the linear separable example), and thus is not sufficient for perfect calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be85e1",
   "metadata": {},
   "source": [
    "### (c) Effect of L2 regularization on model calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e219c5",
   "metadata": {},
   "source": [
    "Add L2 regularization term to the log-likelihood function:\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^m\\left[y^{(i)}\\log(h_\\theta(x^{(i)}))+(1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\right]-\\frac{\\lambda}{2}\\sum_{j=1}^{n-1}\\theta_j^2$$\n",
    "\n",
    "where $\\lambda$ is the weight decay term$.\n",
    "\n",
    "Thus, the partial detivative w.r.t. $\\theta_j$ is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j}l(\\theta)=\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)} - \\lambda\\theta_j,j=0,1,\\dots,n-1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b549ec",
   "metadata": {},
   "source": [
    "Same as (a), let $j=0$, then\n",
    "\n",
    "$$\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)})) - \\lambda\\theta_0=0$$\n",
    "\n",
    "when the log-likelihood is maximized. \n",
    "\n",
    "Therefore, the logistic regression model is no longer well-calibrated."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
