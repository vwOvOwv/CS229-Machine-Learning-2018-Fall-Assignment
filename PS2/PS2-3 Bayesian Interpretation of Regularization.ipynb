{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad3d954",
   "metadata": {},
   "source": [
    "# PS2-3 Bayesian Interpretation of Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cdcc06",
   "metadata": {},
   "source": [
    "### (a) Relation between MAP and MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc0a4fc",
   "metadata": {},
   "source": [
    "From the definition and the chain rule of conditional probability we can obtain\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}&=\\arg\\max_\\theta p(\\theta|x,y)\\\\\n",
    "&=\\arg\\max_\\theta\\frac{p(\\theta,x,y)}{p(x,y)}\\\\\n",
    "&=\\arg\\max_\\theta\\frac{p(y|x,\\theta)p(\\theta|x)p(x)}{p(x,y)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b70a73",
   "metadata": {},
   "source": [
    "$p(x)$ and $p(x,y)$ are constants and we assume $p(\\theta|x)=p(\\theta)$, thus\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}&=\\arg\\max_\\theta p(y|x,\\theta)p(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "while\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MLE}}&=\\arg\\max_\\theta p(y|x;\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0053fb",
   "metadata": {},
   "source": [
    "### (b) MAP estimation with zero-mean Gaussian prior is equivalent to MLE with $L_2$ regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0f324",
   "metadata": {},
   "source": [
    "First, transform maximizing into minimization:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}&=\\arg\\max_\\theta p(y|x,\\theta)p(\\theta)\\\\\n",
    "&=\\arg\\min_\\theta -\\log p(y|x,\\theta) - \\log p(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "$\\theta\\sim\\mathcal{N}(0, \\eta^2I)$, its density is\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\theta) &= \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}\\exp\\left(-\\frac{1}{2}\\theta^T\\Sigma^{-1}\\theta\\right)\\\\\n",
    "&= \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}\\exp\\left(-\\frac{\\theta^T\\theta}{2\\eta^2}\\right)\\\\\n",
    "&= \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}\\exp\\left(-\\frac{||\\theta||_2^2}{2\\eta^2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Thus,\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}&=\\arg\\min_\\theta \\left[-\\log p(y|x,\\theta) - \\left(\\log\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}-\\frac{||\\theta||_2^2}{2\\eta^2}\\right)\\right]\\\\\n",
    "&=\\arg\\min_\\theta \\left[-\\log p(y|x,\\theta) +\\frac{||\\theta||_2^2}{2\\eta^2}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, we can see that MAP estimation with a zero-mean Gaussian prior over $\\theta$ is equivalent to applying $L_2$ regularization with MLE estimation, with $\\lambda=\\frac{1}{2\\eta^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6c81d",
   "metadata": {},
   "source": [
    "### (c) Closed form solution for $θ_{\\text{MAP}}$ with zero-mean Gaussian prior in linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78e8ee",
   "metadata": {},
   "source": [
    "From (b) we know\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}=\\arg\\min_\\theta \\left(-\\log p(y|x,\\theta) +\\frac{||\\theta||_2^2}{2\\eta^2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "p(y|x,\\theta)=\\prod_{i=1}^mp(y^{(i)}|x^{(i)},\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0593c",
   "metadata": {},
   "source": [
    "Since $y^{(i)} = θ^Tx^{(i)} + \\epsilon$, where $\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$ and $\\theta^Tx^{(i)}$ can be viewed as a scalar, from the property of Gaussian distribution we know $y^{(i)}|x^{(i)},\\theta \\sim \\mathcal{N}(\\theta^Tx^{(i)}, \\sigma^2)$. Therefore,\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}&=\\arg\\min_\\theta -\\sum_{i=1}^m\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}\\right\\}+\\frac{||\\theta||_2^2}{2\\eta^2}\\\\\n",
    "&=\\arg\\min_\\theta \\sum_{i=1}^m\\left[\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2} \\right] + \\frac{||\\theta||_2^2}{2\\eta^2}\n",
    "\\end{align*}\n",
    "\n",
    "Next, transform the equation above into a matrix form:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}&=\\arg\\min_\\theta \\frac{1}{2\\sigma^2}\\left(\\vec{y}-X\\theta\\right)^T\\left(\\vec{y}-X\\theta\\right) + \\frac{\\theta^T\\theta}{2\\eta^2}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc68d6",
   "metadata": {},
   "source": [
    "Therefore, the objective function can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\theta) &= \\frac{1}{2\\sigma^2}\\left(\\vec{y}-X\\theta\\right)^T\\left(\\vec{y}-X\\theta\\right) + \\frac{\\theta^T\\theta}{2\\eta^2}\n",
    "\\end{align*}\n",
    "\n",
    "The gradient of $J(\\theta)$ w.r.t. $\\theta$ is\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) &= \\frac{1}{\\sigma^2}（-X^T）\\left(\\vec{y}-X\\theta\\right) + \\frac{1}{\\eta^2}\\theta\\\\\n",
    "&= -\\frac{1}{\\sigma^2}X^T\\vec{y} + \\frac{1}{\\sigma^2}X^TX\\theta + \\frac{1}{\\eta^2}\\theta\n",
    "\\end{align*}\n",
    "\n",
    "Setting the gradient to zero gives us the closed form solution:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta = \\left(X^TX + \\frac{\\sigma^2}{\\eta^2}I\\right)^{-1}X^T\\vec{y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b8a0e",
   "metadata": {},
   "source": [
    "### (d) MAP estimation with zero-mean Laplace prior is equivalent to MLE with $L_1$ regularization.\n",
    "\n",
    "Fron (b) we know\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}=\\arg\\min_\\theta \\left(-\\log p(y|x,\\theta) - \\log p(\\theta)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Here, we assume $\\theta\\sim\\mathcal{L}(0, bI)$, thus the density is\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\theta) = \\frac{1}{(2b)^n}\\exp\\left(-\\frac{||\\theta||_1}{b}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}&=\\arg\\min_\\theta \\left(-\\log p(y|x,\\theta) - \\log \\frac{1}{(2b)^n} + \\frac{||\\theta||_1}{b}\\right)\\\\\n",
    "&=\\arg\\min_\\theta \\left(-\\log p(y|x,\\theta) + \\frac{||\\theta||_1}{b}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, we can see that MAP estimation with a zero-mean Laplace prior over $\\theta$ is equivalent to applying $L_1$ regularization with MLE estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037a00f",
   "metadata": {},
   "source": [
    "Next, consider the linear regression in (c). In this case the original objective function is\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\theta) &= \\frac{1}{2\\sigma^2}\\left(\\vec{y}-X\\theta\\right)^T\\left(\\vec{y}-X\\theta\\right) + \\frac{||\\theta||_1}{b}\n",
    "\\end{align*}\n",
    "\n",
    "Thus, \n",
    "\\begin{align*}\n",
    "\\theta_{\\text{MAP}}&=\\arg\\min_\\theta J(\\theta)\\\\\n",
    "&= \\arg\\min_\\theta \\left(\\vec{y}-X\\theta\\right)^T\\left(\\vec{y}-X\\theta\\right) + \\frac{2\\sigma^2}{b}||\\theta||_1\\\\\n",
    "&= \\arg\\min_\\theta ||X\\theta-\\vec{y}||_2^2 + \\gamma ||\\theta||_1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, we can rewrite $J(\\theta)=||X\\theta-\\vec{y}||_2^2 + \\gamma ||\\theta||_1$, where $\\gamma = \\frac{2\\sigma^2}{b}$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
