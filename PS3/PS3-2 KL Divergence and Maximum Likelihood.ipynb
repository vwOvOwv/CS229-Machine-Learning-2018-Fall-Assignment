{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4c4b33",
   "metadata": {},
   "source": [
    "# PS3-2 KL Divergence and Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e5a0e",
   "metadata": {},
   "source": [
    "### (a) Nonnegativity of KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d395ce",
   "metadata": {},
   "source": [
    "Assume random variable $X\\sim P$, and $P(x)$ denotes the PDF of $P$. $Q$ is another distribution, and the PDF of $P$ and $Q$ (i.e., $P(x)$ and $Q(x)$) are defined on $\\mathcal X$. Then from the properties of expectation we can obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "D_{\\text{KL}}(P||Q)&=\\sum_{x\\in\\mathcal X}P(x)\\log\\frac{P(x)}{Q(x)}\\\\\n",
    "&=\\sum_{x\\in\\mathcal X}P(x)\\left(-\\log\\frac{Q(x)}{P(x)}\\right)\\\\\n",
    "&=\\mathbb E\\left[-\\log\\frac{Q(x)}{P(x)}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Here, $\\frac{Q(x)}{P(x)}$ can be view as a random variable generated by $X$.\n",
    "\n",
    "Since $f(x)=-\\log x$ is convex, from Jensen's inequality we obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb E\\left[-\\log\\frac{Q(x)}{P(x)}\\right]&\\ge-\\log\\mathbb E\\left[\\frac{Q(x)}{P(x)}\\right]\\\\\n",
    "&=-\\log\\sum_{x\\in\\mathcal X}P(x)\\frac{Q(x)}{P(x)}\\\\\n",
    "&=-\\log\\sum_{x\\in\\mathcal X}Q(x)\\\\\n",
    "&=-\\log 1\\\\\n",
    "&=0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2135c21",
   "metadata": {},
   "source": [
    "If $P=Q$, \n",
    "\n",
    "\\begin{align*}\n",
    "D_{\\text{KL}}(P||Q)&=\\mathbb E\\left[-\\log\\frac{Q(x)}{P(x)}\\right]=0.\n",
    "\\end{align*}\n",
    "\n",
    "And if $D_{\\text{KL}}(P||Q)=0$, then $\\mathbb E\\left[-\\log\\frac{Q(x)}{P(x)}\\right]=-\\log\\mathbb E\\left[\\frac{Q(x)}{P(x)}\\right]$, which means\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{Q(x)}{P(x)}=\\mathbb E\\left[\\frac{Q(x)}{P(x)}\\right]=1\n",
    "\\end{align*}\n",
    "\n",
    "and thus $P=Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e56844",
   "metadata": {},
   "source": [
    "### (b) Chain rule for KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03af3e",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "D_{\\text{KL}}\\left(P(X,Y)||Q(X,Y)\\right)&=\\sum_{x}\\sum_{y}P(x,y)\\log\\frac{P(x,y)}{Q(x,y)}\\\\\n",
    "&=\\sum_x\\sum_yP(x,y)\\log\\frac{P(y|x)P(x)}{Q(y|x)Q(x)}\\\\\n",
    "&=\\sum_x\\sum_yP(x,y)\\left[\\log\\frac{P(y|x)}{Q(y|x)}+\\log\\frac{P(x)}{Q(x)}\\right]\\\\\n",
    "&=\\sum_x\\sum_yP(x,y)\\log\\frac{P(y|x)}{Q(y|x)} + \\sum_x\\sum_yP(x,y)\\log\\frac{P(x)}{Q(x)}\\\\\n",
    "&=\\sum_x\\sum_yP(y|x)P(x)\\log\\frac{P(y|x)}{Q(y|x)} + \\sum_x\\left(\\sum_yP(x,y)\\right)\\log\\frac{P(x)}{Q(x)}\\\\\n",
    "&=\\sum_xP(x)\\sum_yP(y|x)\\log\\frac{P(y|x)}{Q(y|x)} + \\sum_xP(x)\\log\\frac{P(x)}{Q(x)}\\\\\n",
    "&=D_{\\text{KL}}(P(Y|X)||Q(Y|X)) + D_{\\text{KL}}(P(X)||Q(X))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf912c",
   "metadata": {},
   "source": [
    "### (c) KL and maximum likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73a57ae",
   "metadata": {},
   "source": [
    "From the definition of KL divergence we have:\n",
    "\\begin{align*}\n",
    "D_{\\text{KL}}\\left(\\hat P||P_{\\theta}\\right)&=\\sum_x\\hat P(x)\\log\\frac{\\hat P(x)}{P_{\\theta}(x)}\\\\\n",
    "&=\\sum_{x}\\hat P(x)\\log\\hat P(x)-\\sum_{x}\\hat P(x)\\log P_{\\theta}(x).\n",
    "\\end{align*}\n",
    "\n",
    "Therefore,\n",
    "\\begin{align*}\n",
    "\\arg\\min_{\\theta}D_{\\text{KL}}\\left(\\hat P||P_{\\theta}\\right)&=\\arg\\max_{\\theta}\\sum_{x}\\hat P(x)\\log P_{\\theta}(x)\\\\\n",
    "&=\\arg\\max_{\\theta}\\sum_{i=1}^m\\hat P(x^{(i)})\\log P_\\theta(x^{(i)}).\n",
    "\\end{align*}\n",
    "\n",
    "Note that $\\hat P(x^{(i)})=\\frac{1}{m}$ for each $i$, thus\n",
    "\\begin{align*}\n",
    "\\arg\\min_{\\theta}D_{\\text{KL}}\\left(\\hat P||P_{\\theta}\\right)&=\\arg\\max_{\\theta}\\frac{1}{m}\\sum_{i=1}^m\\log P_\\theta(x^{(i)})\\\\\n",
    "&=\\arg\\max_{\\theta}\\sum_{i=1}^m\\log P_\\theta(x^{(i)})\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
