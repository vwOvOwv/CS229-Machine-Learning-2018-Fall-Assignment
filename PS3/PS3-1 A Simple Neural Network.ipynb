{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade0c28e",
   "metadata": {},
   "source": [
    "# PS3-1 A Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb3ae1",
   "metadata": {},
   "source": [
    "### (a) Update Rule of $w_{1,2}^{[1]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e56a7",
   "metadata": {},
   "source": [
    "First, formulate the forward pass involving $W_{1,2}^{[1]}$. For sample $x^{(i)}$,\n",
    "\\begin{align*}\n",
    "&z_2^{[1],(i)}=w_{0,2}^{[1]}+w_{1,2}^{[1]}x_1^{(i)}+w_{2,2}^{[1]}x_2^{(i)}\\\\\n",
    "&h_2^{(i)}=\\sigma\\left(z_2^{[1],(i)}\\right)\\\\\n",
    "&z^{[2],(i)}=w_0^{[2]}+w_1^{[2]}h_1^{(i)}+w_2^{[2]}h_2^{(i)}+w_3^{[2]}h_3^{(i)}\\\\\n",
    "&o^{(i)}=\\sigma\\left(z_2^{[2],(i)}\\right)\\\\\n",
    "&l^{(i)}=\\left(o^{(i)}-y^{(i)}\\right)^2\\\\\n",
    "&l=\\frac{1}{m}\\sum_{i=1}^ml^{(i)}\n",
    "\\end{align*}\n",
    "\n",
    "Thus, according to the chain rule:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial{w_{1,2}^{[l]}}}&=\\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial l^{(i)}}{\\partial{w_{1,2}^{[l]}}}\\\\\n",
    "&=\\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial l^{(i)}}{\\partial{o^{(i)}}}\\frac{\\partial o^{(i)}}{\\partial{z^{[2],(i)}}}\\frac{\\partial z^{[2],(i)}}{\\partial{h_2^{(i)}}}\\frac{\\partial h_2^{(i)}}{\\partial{z_2^{[1],(i)}}}\\frac{\\partial z_2^{[1],(i)}}{\\partial{w_{1,2}^{[1]}}}\\\\\n",
    "&=\\frac{2}{m}\\sum_{i=1}^m\\left(o^{(i)}-y^{(i)}\\right)\\sigma\\left(z^{[2],(i)}\\right)\\left(1-\\sigma\\left(z^{[2],(i)}\\right)\\right)w_2^{[2]}\\sigma\\left(z_2^{[1],(i)}\\right)\\left(1-\\sigma\\left(z_2^{[1],(i)}\\right)\\right)x_1^{(i)}\\\\\n",
    "&=\\frac{2}{m}\\sum_{i=1}^m\\left(o^{(i)}-y^{(i)}\\right)o^{(i)}\\left(1-o^{(i)}\\right)w_2^{[2]}h_2^{(i)}\\left(1-h_2^{(i)}\\right)x_1^{(i)}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the update rule of $w_{1,2}^{[1]}$ is\n",
    "\\begin{align*}\n",
    "w_{1,2}^{[1]}&:=w_{1,2}^{[1]}-\\alpha\\frac{\\partial l}{\\partial{w_{1,2}}}\\\\\n",
    "&=w_{1,2}^{[1]}-\\frac{2\\alpha}{m}\\sum_{i=1}^m\\left(o^{(i)}-y^{(i)}\\right)o^{(i)}\\left(1-o^{(i)}\\right)w_2^{[2]}h_2^{(i)}\\left(1-h_2^{(i)}\\right)x_1^{(i)}\n",
    "\\end{align*}\n",
    "where $h_2^{(i)}=\\sigma\\left(w_{0,2}^{[1]}+w_{1,2}^{[1]}x_1^{(i)}+w_{2,2}^{[1]}x_2^{(i)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f9fa7",
   "metadata": {},
   "source": [
    "### (b) Step Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d389cfb",
   "metadata": {},
   "source": [
    "It can be observed from visualization of the dataset that the decision boundary is triangular and consists of\n",
    "\\begin{align*}\n",
    "&l_1:x_1=0.5\\\\\n",
    "&l_2:x_2=0.5\\\\\n",
    "&l_3:x_1+x_2-4=0\n",
    "\\end{align*}\n",
    "\n",
    "Since there're three hidden units, we can view the neural network as a combination of three logitic regressions using step activation function, with each hidden unit performing one logistic regression.\n",
    "\n",
    "Therefore, by setting\n",
    "\n",
    "\\begin{align*}\n",
    "&w_{0,1}^{[1]}=0.5,w_{1,1}^{[1]}=-1,w_{2,1}^{[1]}=0\\\\\n",
    "&w_{0,2}^{[1]}=0.5,w_{1,2}^{[1]}=0,w_{2,1}^{[1]}=-1\\\\\n",
    "&w_{0,3}^{[1]}=-4,w_{1,3}^{[1]}=1,w_{2,3}^{[1]}=1\n",
    "\\end{align*}\n",
    "\n",
    "the decision boundaries of $h_1,h_2,h_3$ are set to $l_1,l_2,l_3$, respectively. And note that $y=0$ if and only if $h_1=h_2=h_3=0$, thus by setting\n",
    "\n",
    "\\begin{align*}\n",
    "w_0^{[2]}=-0.1, w_1^{[2]}=w_2^{[2]}=w_3^{[2]}=1\n",
    "\\end{align*}\n",
    "\n",
    "we should be able to achieve 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2815d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def example_weights():\n",
    "    \"\"\"This is an example function that returns weights.\n",
    "    Use this function as a template for optimal_step_weights and optimal_sigmoid_weights.\n",
    "    You do not need to modify this class for this assignment.\n",
    "    \"\"\"\n",
    "    w = {}\n",
    "\n",
    "    w['hidden_layer_0_1'] = 0\n",
    "    w['hidden_layer_1_1'] = 0\n",
    "    w['hidden_layer_2_1'] = 0\n",
    "    w['hidden_layer_0_2'] = 0\n",
    "    w['hidden_layer_1_2'] = 0\n",
    "    w['hidden_layer_2_2'] = 0\n",
    "    w['hidden_layer_0_3'] = 0\n",
    "    w['hidden_layer_1_3'] = 0\n",
    "    w['hidden_layer_2_3'] = 0\n",
    "\n",
    "    w['output_layer_0'] = 0\n",
    "    w['output_layer_1'] = 0\n",
    "    w['output_layer_2'] = 0\n",
    "    w['output_layer_3'] = 0\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def optimal_step_weights():\n",
    "    \"\"\"Return the optimal weights for the neural network with a step activation function.\n",
    "    \n",
    "    This function will not be graded if there are no optimal weights.\n",
    "    See the PDF for instructions on what each weight represents.\n",
    "    \n",
    "    The hidden layer weights are notated by [1] on the problem set and \n",
    "    the output layer weights are notated by [2].\n",
    "\n",
    "    This function should return a dict with elements for each weight, see example_weights above.\n",
    "\n",
    "    \"\"\"\n",
    "    w = example_weights()\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    w['hidden_layer_0_1'] = 0.5\n",
    "    w['hidden_layer_1_1'] = -1\n",
    "    w['hidden_layer_2_1'] = 0\n",
    "    w['hidden_layer_0_2'] = 0.5\n",
    "    w['hidden_layer_1_2'] = 0\n",
    "    w['hidden_layer_2_2'] = -1\n",
    "    w['hidden_layer_0_3'] = -4\n",
    "    w['hidden_layer_1_3'] = 1\n",
    "    w['hidden_layer_2_3'] = 1\n",
    "\n",
    "    w['output_layer_0'] = -0.1\n",
    "    w['output_layer_1'] = 1\n",
    "    w['output_layer_2'] = 1\n",
    "    w['output_layer_3'] = 1\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    return w\n",
    "\n",
    "def optimal_linear_weights():\n",
    "    \"\"\"Return the optimal weights for the neural network with a linear activation function for the hidden units.\n",
    "    \n",
    "    This function will not be graded if there are no optimal weights.\n",
    "    See the PDF for instructions on what each weight represents.\n",
    "    \n",
    "    The hidden layer weights are notated by [1] on the problem set and \n",
    "    the output layer weights are notated by [2].\n",
    "\n",
    "    This function should return a dict with elements for each weight, see example_weights above.\n",
    "\n",
    "    \"\"\"\n",
    "    w = example_weights()\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    return w\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    step_weights = optimal_step_weights()\n",
    "\n",
    "    with open('output/step_weights', 'w') as f:\n",
    "        json.dump(step_weights, f)\n",
    "\n",
    "    linear_weights = optimal_linear_weights()\n",
    "\n",
    "    with open('output/linear_weights', 'w') as f:\n",
    "        json.dump(linear_weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38584bd1",
   "metadata": {},
   "source": [
    "### (c) Linear Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b18eb",
   "metadata": {},
   "source": [
    "When the activation function is linear, computation performed by the hidden units is linear transformation. And $o$ performs a single logistic regression using step activation function. Thus, the neural network will only fit a linear decision boundary and not be able to fit the triangular decision boundary of the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_snn_ei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
