{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2239ab6c",
   "metadata": {},
   "source": [
    "# PS4-1 Neural Networks: MNIST Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549fcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "MAX_POOL_SIZE = 5\n",
    "CONVOLUTION_SIZE = 4\n",
    "CONVOLUTION_FILTERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431226a",
   "metadata": {},
   "source": [
    "### (a) Implement backward function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981236b",
   "metadata": {},
   "source": [
    "Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d74b000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for a single example.\n",
    "    The shape of the input is of size # num classes.\n",
    "\n",
    "    Important Note: You must be careful to avoid overflow for this function. Functions\n",
    "    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n",
    "    You will know that your function is overflow resistent when it can handle input like:\n",
    "    np.array([[10000, 10010, 10]]) without issues.\n",
    "\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array containing the softmax results of shape  number_of_classes\n",
    "    \"\"\"\n",
    "    x = x - np.max(x, axis=0)\n",
    "    exp = np.exp(x)\n",
    "    s = exp / np.sum(exp, axis=0)\n",
    "    return s\n",
    "\n",
    "def backward_softmax(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x.\n",
    "\n",
    "    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n",
    "\n",
    "    Args:\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "        grad_outputs: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n",
    "    \"\"\"\n",
    "    \n",
    "    # *** START CODE HERE ***\n",
    "    softmax_x = forward_softmax(x)\n",
    "    dot_product = np.dot(grad_outputs, softmax_x)\n",
    "    grad_x = softmax_x * (grad_outputs - dot_product)\n",
    "    return grad_x\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937cd61",
   "metadata": {},
   "source": [
    "ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a663db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu function for the input x.\n",
    "\n",
    "    Args:\n",
    "        x: A numpy float array\n",
    "\n",
    "    Returns:\n",
    "        A numpy float array containing the relu results\n",
    "    \"\"\"\n",
    "    \n",
    "    x[x<=0] = 0\n",
    "    return x\n",
    "\n",
    "def backward_relu(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x\n",
    "\n",
    "    Args:\n",
    "        x: A numpy array of arbitrary shape containing the input.\n",
    "        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n",
    "            to the output of relu\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of the same shape as x containing the gradients with respect to x.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    grad_x = grad_outputs * (x > 0).astype(float)\n",
    "    return grad_x\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005fb8fe",
   "metadata": {},
   "source": [
    "Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f30498c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the output from a cross entropy loss layer given the probabilities and labels.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be a scalar\n",
    "\n",
    "    Returns:\n",
    "        The result of the log loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    result = 0\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            result += -np.log(probabilities[i])\n",
    "\n",
    "    return result\n",
    "\n",
    "def backward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross entropy loss with respect to the probabilities.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be the gradient with respect to the probabilities.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    grad_prob = np.zeros_like(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            grad_prob[i] = -1 / probabilities[i]\n",
    "    return grad_prob\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe88264",
   "metadata": {},
   "source": [
    "Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a17be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear(weights, bias, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a linear layer with the given weights, bias and data.\n",
    "    weights is of the shape (input # features, output # features)\n",
    "    bias is of the shape (output # features)\n",
    "    data is of the shape (input # features)\n",
    "\n",
    "    The output should be of the shape (output # features)\n",
    "\n",
    "    Returns:\n",
    "        The result of the linear layer\n",
    "    \"\"\"\n",
    "    return data.dot(weights) + bias\n",
    "\n",
    "def backward_linear(weights, bias, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the loss with respect to the parameters of a linear layer.\n",
    "\n",
    "    See forward_linear for information about the shapes of the variables.\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of this layer.\n",
    "\n",
    "    This should return a tuple with three elements:\n",
    "    - The gradient of the loss with respect to the weights\n",
    "    - The gradient of the loss with respect to the bias\n",
    "    - The gradient of the loss with respect to the data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    grad_data = np.dot(output_grad, weights.T)\n",
    "    grad_weights = np.outer(data, output_grad)\n",
    "    grad_bias = output_grad\n",
    "    return grad_weights, grad_bias, grad_data\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9f793",
   "metadata": {},
   "source": [
    "2-D Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4baeb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = np.sum(\n",
    "                    np.multiply(data[:, x:(x + conv_width), y:(y + conv_height)], conv_W[output_channel, :, :, :])) + conv_b[output_channel]\n",
    "\n",
    "    return output\n",
    "\n",
    "def backward_convolution(conv_W, conv_b, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the parameters of the convolution.\n",
    "\n",
    "    See forward_convolution for the sizes of the arguments.\n",
    "    output_grad is the gradient of the loss with respect to the output of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing 3 gradients.\n",
    "        The first element is the gradient of the loss with respect to the convolution weights\n",
    "        The second element is the gradient of the loss with respect to the convolution bias\n",
    "        The third element is the gradient of the loss with respect to the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    output_width, output_height = input_width - conv_width + 1, input_height - conv_height + 1\n",
    "\n",
    "    grad_w = np.zeros_like(conv_W)\n",
    "    grad_data = np.zeros_like(data)\n",
    "    for x in range(output_width):\n",
    "        for y in range(output_height):\n",
    "            for output_channel in range(conv_channels):\n",
    "                grad_w[output_channel] += output_grad[output_channel, x, y] * data[:, x: x + conv_width, y: y + conv_height]\n",
    "                grad_data[:, x: x + conv_width, y: y + conv_height] += output_grad[output_channel, x, y] * conv_W[output_channel]\n",
    "    \n",
    "    grad_b = np.sum(output_grad, axis=(1, 2))\n",
    "\n",
    "    return grad_w, grad_b, grad_data\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76ac53",
   "metadata": {},
   "source": [
    "Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e04c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_max_pool(data, pool_width, pool_height):\n",
    "    \"\"\"\n",
    "    Compute the output from a max pooling layer given the data and pool dimensions.\n",
    "\n",
    "    The stride length should be equal to the pool size\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "\n",
    "    The output should be the result of the max pooling layer and should be of size:\n",
    "        (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    Returns:\n",
    "        The result of the max pooling layer\n",
    "    \"\"\"\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    \n",
    "    output = np.zeros((input_channels, input_width // pool_width, input_height // pool_height))\n",
    "\n",
    "    for x in range(0, input_width, pool_width):\n",
    "        for y in range(0, input_height, pool_height):\n",
    "\n",
    "            output[:, x // pool_width, y // pool_height] = np.amax(data[:, x:(x + pool_width), y:(y + pool_height)], axis=(1, 2))\n",
    "\n",
    "    return output\n",
    "\n",
    "def backward_max_pool(data, pool_width, pool_height, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the data in the max pooling layer.\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of the backward max\n",
    "    pool layer.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the data (of same shape as data)\n",
    "    \"\"\"\n",
    "    \n",
    "    # *** START CODE HERE ***\n",
    "    num_channels, width, height = data.shape\n",
    "    output_width, output_height = width // pool_width, height // pool_height\n",
    "\n",
    "    grad_data = np.zeros_like(data)\n",
    "    for x in range(output_width):\n",
    "        for y in range(output_height):\n",
    "            for channel in range(num_channels):\n",
    "                start_x = x * pool_width\n",
    "                end_x = start_x + pool_width\n",
    "                start_y = y * pool_height\n",
    "                end_y = start_y + pool_height\n",
    "                pool_window = data[channel, start_x: end_x, start_y: end_y]\n",
    "                idx = np.unravel_index(np.argmax(pool_window), pool_window.shape)\n",
    "                grad_data[channel, start_x + idx[0], start_y + idx[1]] = output_grad[channel, x, y]\n",
    "    \n",
    "    return grad_data\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e3d90",
   "metadata": {},
   "source": [
    "### (b) Implement backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "121bfb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_params():\n",
    "    \"\"\"\n",
    "    Compute the initial parameters for the neural network.\n",
    "\n",
    "    This function should return a dictionary mapping parameter names to numpy arrays containing\n",
    "    the initial values for those parameters.\n",
    "\n",
    "    There should be four parameters for this model:\n",
    "    W1 is the weight matrix for the convolutional layer\n",
    "    b1 is the bias vector for the convolutional layer\n",
    "    W2 is the weight matrix for the output layers\n",
    "    b2 is the bias vector for the output layer\n",
    "\n",
    "    Weight matrices should be initialized with values drawn from a random normal distribution.\n",
    "    The mean of that distribution should be 0.\n",
    "    The variance of that distribution should be 1/sqrt(n) where n is the number of neurons that\n",
    "    feed into an output for that layer.\n",
    "\n",
    "    Bias vectors should be initialized with zero.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        A dict mapping parameter names to numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    size_after_convolution = 28 - CONVOLUTION_SIZE + 1\n",
    "    size_after_max_pooling = size_after_convolution // MAX_POOL_SIZE\n",
    "\n",
    "    num_hidden = size_after_max_pooling * size_after_max_pooling * CONVOLUTION_FILTERS\n",
    "\n",
    "    return {\n",
    "        'W1': np.random.normal(size = (CONVOLUTION_FILTERS, 1, CONVOLUTION_SIZE, CONVOLUTION_SIZE), scale=1/ math.sqrt(CONVOLUTION_SIZE * CONVOLUTION_SIZE)),\n",
    "        'b1': np.zeros(CONVOLUTION_FILTERS),\n",
    "        'W2': np.random.normal(size = (num_hidden, 10), scale = 1/ math.sqrt(num_hidden)),\n",
    "        'b2': np.zeros(10)\n",
    "    }\n",
    "\n",
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the forward layer given the data, labels, and params.\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input (shape is 1 by 28 by 28)\n",
    "        labels: A 1d numpy array containing the labels (shape is 10)\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A 2 element tuple containing:\n",
    "            1. A numpy array The output (after the softmax) of the output layer\n",
    "            2. The average loss for these data elements\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "    \n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "    cost = forward_cross_entropy_loss(y, labels)\n",
    "\n",
    "    return y, cost\n",
    "\n",
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation gradient computation step for a neural network\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input for a single example\n",
    "        labels: A 1d numpy array containing the labels for a single example\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2, and b2\n",
    "            W1 and b1 represent the weights and bias for the convolutional layer\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of strings to numpy arrays where each key represents the name of a weight\n",
    "        and the values represent the gradient of the loss with respect to that weight.\n",
    "        \n",
    "        In particular, it should have 4 elements:\n",
    "            W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "    \n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "\n",
    "    grad_y = backward_cross_entropy_loss(y, labels)\n",
    "    grad_logits = backward_softmax(logits, grad_y)\n",
    "    grad_W2, grad_b2, grad_flattened = backward_linear(W2, b2, flattened, grad_logits)\n",
    "    grad_first_after_relu = grad_flattened.reshape(first_after_relu.shape)\n",
    "    grad_first_max_pool = backward_relu(first_max_pool, grad_first_after_relu)\n",
    "    grad_first_convolution = backward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE, grad_first_max_pool)\n",
    "    grad_W1, grad_b1, grad_data = backward_convolution(W1, b1, data, grad_first_convolution)\n",
    "\n",
    "    return {'W1': grad_W1, 'W2': grad_W2, 'b1': grad_b1, 'b2': grad_b2}\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "def forward_prop_batch(batch_data, batch_labels, params, forward_prop_func):\n",
    "    \"\"\"Apply the forward prop func to every image in a batch\"\"\"\n",
    "\n",
    "    y_array = []\n",
    "    cost_array = []\n",
    "\n",
    "    for item, label in zip(batch_data, batch_labels):\n",
    "        y, cost = forward_prop_func(item, label, params)\n",
    "        y_array.append(y)\n",
    "        cost_array.append(cost)\n",
    "\n",
    "    return np.array(y_array), np.array(cost_array)\n",
    "\n",
    "def gradient_descent_batch(batch_data, batch_labels, learning_rate, params, backward_prop_func):\n",
    "    \"\"\"\n",
    "    Perform one batch of gradient descent on the given training data using the provided learning rate.\n",
    "\n",
    "    This code should update the parameters stored in params.\n",
    "    It should not return anything\n",
    "\n",
    "    Args:\n",
    "        batch_data: A numpy array containing the training data for the batch\n",
    "        train_labels: A numpy array containing the training labels for the batch\n",
    "        learning_rate: The learning rate\n",
    "        params: A dict of parameter names to parameter values that should be updated.\n",
    "        backward_prop_func: A function that follows the backwards_prop API\n",
    "\n",
    "    Returns: This function returns nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    total_grad = {}\n",
    "\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        grad = backward_prop_func(\n",
    "            batch_data[i, :, :],\n",
    "            batch_labels[i, :],\n",
    "            params)\n",
    "        for key, value in grad.items():\n",
    "            if key not in total_grad:\n",
    "                total_grad[key] = np.zeros(value.shape)\n",
    "\n",
    "            total_grad[key] += value\n",
    "\n",
    "    params['W1'] = params['W1'] - learning_rate * total_grad['W1']\n",
    "    params['W2'] = params['W2'] - learning_rate * total_grad['W2']\n",
    "    params['b1'] = params['b1'] - learning_rate * total_grad['b1']\n",
    "    params['b2'] = params['b2'] - learning_rate * total_grad['b2']\n",
    "\n",
    "    # This function does not return anything\n",
    "    return\n",
    "\n",
    "def nn_train(\n",
    "    train_data, train_labels, dev_data, dev_labels,\n",
    "    get_initial_params_func, forward_prop_func, backward_prop_func,\n",
    "    learning_rate=5.0, batch_size=16, num_batches=400):\n",
    "\n",
    "    m = train_data.shape[0]\n",
    "\n",
    "    params = get_initial_params_func()\n",
    "\n",
    "    cost_dev = []\n",
    "    accuracy_dev = []\n",
    "    for batch in range(num_batches):\n",
    "        print('Currently processing {} / {}'.format(batch, num_batches))\n",
    "\n",
    "        batch_data = train_data[batch * batch_size:(batch + 1) * batch_size, :, :, :]\n",
    "        batch_labels = train_labels[batch * batch_size: (batch + 1) * batch_size, :]\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            output, cost = forward_prop_batch(dev_data, dev_labels, params, forward_prop_func)\n",
    "            cost_dev.append(sum(cost) / len(cost))\n",
    "            accuracy_dev.append(compute_accuracy(output, dev_labels))\n",
    "\n",
    "            print('Cost and accuracy', cost_dev[-1], accuracy_dev[-1])\n",
    "\n",
    "        gradient_descent_batch(batch_data, batch_labels,\n",
    "            learning_rate, params, backward_prop_func)\n",
    "\n",
    "    return params, cost_dev, accuracy_dev\n",
    "\n",
    "def nn_test(data, labels, params):\n",
    "    output, cost = forward_prop(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    correct_output = np.argmax(output,axis=1)\n",
    "    correct_labels = np.argmax(labels,axis=1)\n",
    "\n",
    "    is_correct = [a == b for a,b in zip(correct_output, correct_labels)]\n",
    "\n",
    "    accuracy = sum(is_correct) * 1. / labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def read_data(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "\n",
    "    x = np.reshape(x, (x.shape[0], 1, 28, 28))\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def run_train(all_data, all_labels, backward_prop_func):\n",
    "    params, cost_dev, accuracy_dev = nn_train(\n",
    "        all_data['train'], all_labels['train'],\n",
    "        all_data['dev'], all_labels['dev'],\n",
    "        get_initial_params, forward_prop, backward_prop_func,\n",
    "        learning_rate=1e-2, batch_size=16, num_batches=400\n",
    "    )\n",
    "\n",
    "    t = np.arange(400 // 100)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    ax1.plot(t, cost_dev, 'b')\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_title('Training curve')\n",
    "\n",
    "    ax2.plot(t, accuracy_dev, 'b')\n",
    "    ax2.set_xlabel('time')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "\n",
    "    fig.savefig('output/train.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c3d09d",
   "metadata": {},
   "source": [
    "### Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba386e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing 0 / 400\n",
      "Cost and accuracy 2.721417647426753 0.0725\n",
      "Currently processing 1 / 400\n",
      "Currently processing 2 / 400\n",
      "Currently processing 3 / 400\n",
      "Currently processing 4 / 400\n",
      "Currently processing 5 / 400\n",
      "Currently processing 6 / 400\n",
      "Currently processing 7 / 400\n",
      "Currently processing 8 / 400\n",
      "Currently processing 9 / 400\n",
      "Currently processing 10 / 400\n",
      "Currently processing 11 / 400\n",
      "Currently processing 12 / 400\n",
      "Currently processing 13 / 400\n",
      "Currently processing 14 / 400\n",
      "Currently processing 15 / 400\n",
      "Currently processing 16 / 400\n",
      "Currently processing 17 / 400\n",
      "Currently processing 18 / 400\n",
      "Currently processing 19 / 400\n",
      "Currently processing 20 / 400\n",
      "Currently processing 21 / 400\n",
      "Currently processing 22 / 400\n",
      "Currently processing 23 / 400\n",
      "Currently processing 24 / 400\n",
      "Currently processing 25 / 400\n",
      "Currently processing 26 / 400\n",
      "Currently processing 27 / 400\n",
      "Currently processing 28 / 400\n",
      "Currently processing 29 / 400\n",
      "Currently processing 30 / 400\n",
      "Currently processing 31 / 400\n",
      "Currently processing 32 / 400\n",
      "Currently processing 33 / 400\n",
      "Currently processing 34 / 400\n",
      "Currently processing 35 / 400\n",
      "Currently processing 36 / 400\n",
      "Currently processing 37 / 400\n",
      "Currently processing 38 / 400\n",
      "Currently processing 39 / 400\n",
      "Currently processing 40 / 400\n",
      "Currently processing 41 / 400\n",
      "Currently processing 42 / 400\n",
      "Currently processing 43 / 400\n",
      "Currently processing 44 / 400\n",
      "Currently processing 45 / 400\n",
      "Currently processing 46 / 400\n",
      "Currently processing 47 / 400\n",
      "Currently processing 48 / 400\n",
      "Currently processing 49 / 400\n",
      "Currently processing 50 / 400\n",
      "Currently processing 51 / 400\n",
      "Currently processing 52 / 400\n",
      "Currently processing 53 / 400\n",
      "Currently processing 54 / 400\n",
      "Currently processing 55 / 400\n",
      "Currently processing 56 / 400\n",
      "Currently processing 57 / 400\n",
      "Currently processing 58 / 400\n",
      "Currently processing 59 / 400\n",
      "Currently processing 60 / 400\n",
      "Currently processing 61 / 400\n",
      "Currently processing 62 / 400\n",
      "Currently processing 63 / 400\n",
      "Currently processing 64 / 400\n",
      "Currently processing 65 / 400\n",
      "Currently processing 66 / 400\n",
      "Currently processing 67 / 400\n",
      "Currently processing 68 / 400\n",
      "Currently processing 69 / 400\n",
      "Currently processing 70 / 400\n",
      "Currently processing 71 / 400\n",
      "Currently processing 72 / 400\n",
      "Currently processing 73 / 400\n",
      "Currently processing 74 / 400\n",
      "Currently processing 75 / 400\n",
      "Currently processing 76 / 400\n",
      "Currently processing 77 / 400\n",
      "Currently processing 78 / 400\n",
      "Currently processing 79 / 400\n",
      "Currently processing 80 / 400\n",
      "Currently processing 81 / 400\n",
      "Currently processing 82 / 400\n",
      "Currently processing 83 / 400\n",
      "Currently processing 84 / 400\n",
      "Currently processing 85 / 400\n",
      "Currently processing 86 / 400\n",
      "Currently processing 87 / 400\n",
      "Currently processing 88 / 400\n",
      "Currently processing 89 / 400\n",
      "Currently processing 90 / 400\n",
      "Currently processing 91 / 400\n",
      "Currently processing 92 / 400\n",
      "Currently processing 93 / 400\n",
      "Currently processing 94 / 400\n",
      "Currently processing 95 / 400\n",
      "Currently processing 96 / 400\n",
      "Currently processing 97 / 400\n",
      "Currently processing 98 / 400\n",
      "Currently processing 99 / 400\n",
      "Currently processing 100 / 400\n",
      "Cost and accuracy 0.6413721697623075 0.78\n",
      "Currently processing 101 / 400\n",
      "Currently processing 102 / 400\n",
      "Currently processing 103 / 400\n",
      "Currently processing 104 / 400\n",
      "Currently processing 105 / 400\n",
      "Currently processing 106 / 400\n",
      "Currently processing 107 / 400\n",
      "Currently processing 108 / 400\n",
      "Currently processing 109 / 400\n",
      "Currently processing 110 / 400\n",
      "Currently processing 111 / 400\n",
      "Currently processing 112 / 400\n",
      "Currently processing 113 / 400\n",
      "Currently processing 114 / 400\n",
      "Currently processing 115 / 400\n",
      "Currently processing 116 / 400\n",
      "Currently processing 117 / 400\n",
      "Currently processing 118 / 400\n",
      "Currently processing 119 / 400\n",
      "Currently processing 120 / 400\n",
      "Currently processing 121 / 400\n",
      "Currently processing 122 / 400\n",
      "Currently processing 123 / 400\n",
      "Currently processing 124 / 400\n",
      "Currently processing 125 / 400\n",
      "Currently processing 126 / 400\n",
      "Currently processing 127 / 400\n",
      "Currently processing 128 / 400\n",
      "Currently processing 129 / 400\n",
      "Currently processing 130 / 400\n",
      "Currently processing 131 / 400\n",
      "Currently processing 132 / 400\n",
      "Currently processing 133 / 400\n",
      "Currently processing 134 / 400\n",
      "Currently processing 135 / 400\n",
      "Currently processing 136 / 400\n",
      "Currently processing 137 / 400\n",
      "Currently processing 138 / 400\n",
      "Currently processing 139 / 400\n",
      "Currently processing 140 / 400\n",
      "Currently processing 141 / 400\n",
      "Currently processing 142 / 400\n",
      "Currently processing 143 / 400\n",
      "Currently processing 144 / 400\n",
      "Currently processing 145 / 400\n",
      "Currently processing 146 / 400\n",
      "Currently processing 147 / 400\n",
      "Currently processing 148 / 400\n",
      "Currently processing 149 / 400\n",
      "Currently processing 150 / 400\n",
      "Currently processing 151 / 400\n",
      "Currently processing 152 / 400\n",
      "Currently processing 153 / 400\n",
      "Currently processing 154 / 400\n",
      "Currently processing 155 / 400\n",
      "Currently processing 156 / 400\n",
      "Currently processing 157 / 400\n",
      "Currently processing 158 / 400\n",
      "Currently processing 159 / 400\n",
      "Currently processing 160 / 400\n",
      "Currently processing 161 / 400\n",
      "Currently processing 162 / 400\n",
      "Currently processing 163 / 400\n",
      "Currently processing 164 / 400\n",
      "Currently processing 165 / 400\n",
      "Currently processing 166 / 400\n",
      "Currently processing 167 / 400\n",
      "Currently processing 168 / 400\n",
      "Currently processing 169 / 400\n",
      "Currently processing 170 / 400\n",
      "Currently processing 171 / 400\n",
      "Currently processing 172 / 400\n",
      "Currently processing 173 / 400\n",
      "Currently processing 174 / 400\n",
      "Currently processing 175 / 400\n",
      "Currently processing 176 / 400\n",
      "Currently processing 177 / 400\n",
      "Currently processing 178 / 400\n",
      "Currently processing 179 / 400\n",
      "Currently processing 180 / 400\n",
      "Currently processing 181 / 400\n",
      "Currently processing 182 / 400\n",
      "Currently processing 183 / 400\n",
      "Currently processing 184 / 400\n",
      "Currently processing 185 / 400\n",
      "Currently processing 186 / 400\n",
      "Currently processing 187 / 400\n",
      "Currently processing 188 / 400\n",
      "Currently processing 189 / 400\n",
      "Currently processing 190 / 400\n",
      "Currently processing 191 / 400\n",
      "Currently processing 192 / 400\n",
      "Currently processing 193 / 400\n",
      "Currently processing 194 / 400\n",
      "Currently processing 195 / 400\n",
      "Currently processing 196 / 400\n",
      "Currently processing 197 / 400\n",
      "Currently processing 198 / 400\n",
      "Currently processing 199 / 400\n",
      "Currently processing 200 / 400\n",
      "Cost and accuracy 0.43482822210982586 0.8625\n",
      "Currently processing 201 / 400\n",
      "Currently processing 202 / 400\n",
      "Currently processing 203 / 400\n",
      "Currently processing 204 / 400\n",
      "Currently processing 205 / 400\n",
      "Currently processing 206 / 400\n",
      "Currently processing 207 / 400\n",
      "Currently processing 208 / 400\n",
      "Currently processing 209 / 400\n",
      "Currently processing 210 / 400\n",
      "Currently processing 211 / 400\n",
      "Currently processing 212 / 400\n",
      "Currently processing 213 / 400\n",
      "Currently processing 214 / 400\n",
      "Currently processing 215 / 400\n",
      "Currently processing 216 / 400\n",
      "Currently processing 217 / 400\n",
      "Currently processing 218 / 400\n",
      "Currently processing 219 / 400\n",
      "Currently processing 220 / 400\n",
      "Currently processing 221 / 400\n",
      "Currently processing 222 / 400\n",
      "Currently processing 223 / 400\n",
      "Currently processing 224 / 400\n",
      "Currently processing 225 / 400\n",
      "Currently processing 226 / 400\n",
      "Currently processing 227 / 400\n",
      "Currently processing 228 / 400\n",
      "Currently processing 229 / 400\n",
      "Currently processing 230 / 400\n",
      "Currently processing 231 / 400\n",
      "Currently processing 232 / 400\n",
      "Currently processing 233 / 400\n",
      "Currently processing 234 / 400\n",
      "Currently processing 235 / 400\n",
      "Currently processing 236 / 400\n",
      "Currently processing 237 / 400\n",
      "Currently processing 238 / 400\n",
      "Currently processing 239 / 400\n",
      "Currently processing 240 / 400\n",
      "Currently processing 241 / 400\n",
      "Currently processing 242 / 400\n",
      "Currently processing 243 / 400\n",
      "Currently processing 244 / 400\n",
      "Currently processing 245 / 400\n",
      "Currently processing 246 / 400\n",
      "Currently processing 247 / 400\n",
      "Currently processing 248 / 400\n",
      "Currently processing 249 / 400\n",
      "Currently processing 250 / 400\n",
      "Currently processing 251 / 400\n",
      "Currently processing 252 / 400\n",
      "Currently processing 253 / 400\n",
      "Currently processing 254 / 400\n",
      "Currently processing 255 / 400\n",
      "Currently processing 256 / 400\n",
      "Currently processing 257 / 400\n",
      "Currently processing 258 / 400\n",
      "Currently processing 259 / 400\n",
      "Currently processing 260 / 400\n",
      "Currently processing 261 / 400\n",
      "Currently processing 262 / 400\n",
      "Currently processing 263 / 400\n",
      "Currently processing 264 / 400\n",
      "Currently processing 265 / 400\n",
      "Currently processing 266 / 400\n",
      "Currently processing 267 / 400\n",
      "Currently processing 268 / 400\n",
      "Currently processing 269 / 400\n",
      "Currently processing 270 / 400\n",
      "Currently processing 271 / 400\n",
      "Currently processing 272 / 400\n",
      "Currently processing 273 / 400\n",
      "Currently processing 274 / 400\n",
      "Currently processing 275 / 400\n",
      "Currently processing 276 / 400\n",
      "Currently processing 277 / 400\n",
      "Currently processing 278 / 400\n",
      "Currently processing 279 / 400\n",
      "Currently processing 280 / 400\n",
      "Currently processing 281 / 400\n",
      "Currently processing 282 / 400\n",
      "Currently processing 283 / 400\n",
      "Currently processing 284 / 400\n",
      "Currently processing 285 / 400\n",
      "Currently processing 286 / 400\n",
      "Currently processing 287 / 400\n",
      "Currently processing 288 / 400\n",
      "Currently processing 289 / 400\n",
      "Currently processing 290 / 400\n",
      "Currently processing 291 / 400\n",
      "Currently processing 292 / 400\n",
      "Currently processing 293 / 400\n",
      "Currently processing 294 / 400\n",
      "Currently processing 295 / 400\n",
      "Currently processing 296 / 400\n",
      "Currently processing 297 / 400\n",
      "Currently processing 298 / 400\n",
      "Currently processing 299 / 400\n",
      "Currently processing 300 / 400\n",
      "Cost and accuracy 0.3673995260534303 0.87\n",
      "Currently processing 301 / 400\n",
      "Currently processing 302 / 400\n",
      "Currently processing 303 / 400\n",
      "Currently processing 304 / 400\n",
      "Currently processing 305 / 400\n",
      "Currently processing 306 / 400\n",
      "Currently processing 307 / 400\n",
      "Currently processing 308 / 400\n",
      "Currently processing 309 / 400\n",
      "Currently processing 310 / 400\n",
      "Currently processing 311 / 400\n",
      "Currently processing 312 / 400\n",
      "Currently processing 313 / 400\n",
      "Currently processing 314 / 400\n",
      "Currently processing 315 / 400\n",
      "Currently processing 316 / 400\n",
      "Currently processing 317 / 400\n",
      "Currently processing 318 / 400\n",
      "Currently processing 319 / 400\n",
      "Currently processing 320 / 400\n",
      "Currently processing 321 / 400\n",
      "Currently processing 322 / 400\n",
      "Currently processing 323 / 400\n",
      "Currently processing 324 / 400\n",
      "Currently processing 325 / 400\n",
      "Currently processing 326 / 400\n",
      "Currently processing 327 / 400\n",
      "Currently processing 328 / 400\n",
      "Currently processing 329 / 400\n",
      "Currently processing 330 / 400\n",
      "Currently processing 331 / 400\n",
      "Currently processing 332 / 400\n",
      "Currently processing 333 / 400\n",
      "Currently processing 334 / 400\n",
      "Currently processing 335 / 400\n",
      "Currently processing 336 / 400\n",
      "Currently processing 337 / 400\n",
      "Currently processing 338 / 400\n",
      "Currently processing 339 / 400\n",
      "Currently processing 340 / 400\n",
      "Currently processing 341 / 400\n",
      "Currently processing 342 / 400\n",
      "Currently processing 343 / 400\n",
      "Currently processing 344 / 400\n",
      "Currently processing 345 / 400\n",
      "Currently processing 346 / 400\n",
      "Currently processing 347 / 400\n",
      "Currently processing 348 / 400\n",
      "Currently processing 349 / 400\n",
      "Currently processing 350 / 400\n",
      "Currently processing 351 / 400\n",
      "Currently processing 352 / 400\n",
      "Currently processing 353 / 400\n",
      "Currently processing 354 / 400\n",
      "Currently processing 355 / 400\n",
      "Currently processing 356 / 400\n",
      "Currently processing 357 / 400\n",
      "Currently processing 358 / 400\n",
      "Currently processing 359 / 400\n",
      "Currently processing 360 / 400\n",
      "Currently processing 361 / 400\n",
      "Currently processing 362 / 400\n",
      "Currently processing 363 / 400\n",
      "Currently processing 364 / 400\n",
      "Currently processing 365 / 400\n",
      "Currently processing 366 / 400\n",
      "Currently processing 367 / 400\n",
      "Currently processing 368 / 400\n",
      "Currently processing 369 / 400\n",
      "Currently processing 370 / 400\n",
      "Currently processing 371 / 400\n",
      "Currently processing 372 / 400\n",
      "Currently processing 373 / 400\n",
      "Currently processing 374 / 400\n",
      "Currently processing 375 / 400\n",
      "Currently processing 376 / 400\n",
      "Currently processing 377 / 400\n",
      "Currently processing 378 / 400\n",
      "Currently processing 379 / 400\n",
      "Currently processing 380 / 400\n",
      "Currently processing 381 / 400\n",
      "Currently processing 382 / 400\n",
      "Currently processing 383 / 400\n",
      "Currently processing 384 / 400\n",
      "Currently processing 385 / 400\n",
      "Currently processing 386 / 400\n",
      "Currently processing 387 / 400\n",
      "Currently processing 388 / 400\n",
      "Currently processing 389 / 400\n",
      "Currently processing 390 / 400\n",
      "Currently processing 391 / 400\n",
      "Currently processing 392 / 400\n",
      "Currently processing 393 / 400\n",
      "Currently processing 394 / 400\n",
      "Currently processing 395 / 400\n",
      "Currently processing 396 / 400\n",
      "Currently processing 397 / 400\n",
      "Currently processing 398 / 400\n",
      "Currently processing 399 / 400\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "train_data, train_labels = read_data('./data/images_train.csv', './data/labels_train.csv')\n",
    "train_labels = one_hot_labels(train_labels)\n",
    "p = np.random.permutation(60000)\n",
    "train_data = train_data[p,:]\n",
    "train_labels = train_labels[p,:]\n",
    "\n",
    "dev_data = train_data[0:400,:]\n",
    "dev_labels = train_labels[0:400,:]\n",
    "train_data = train_data[400:,:]\n",
    "train_labels = train_labels[400:,:]\n",
    "\n",
    "mean = np.mean(train_data)\n",
    "std = np.std(train_data)\n",
    "train_data = (train_data - mean) / std\n",
    "dev_data = (dev_data - mean) / std\n",
    "\n",
    "all_data = {\n",
    "    'train': train_data,\n",
    "    'dev': dev_data,\n",
    "}\n",
    "\n",
    "all_labels = {\n",
    "    'train': train_labels,\n",
    "    'dev': dev_labels,\n",
    "}\n",
    "    \n",
    "run_train(all_data, all_labels, backward_prop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_snn_ei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
