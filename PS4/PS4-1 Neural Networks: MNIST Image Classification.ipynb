{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2239ab6c",
   "metadata": {},
   "source": [
    "# PS4-1 Neural Networks: MNIST Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "549fcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "MAX_POOL_SIZE = 5\n",
    "CONVOLUTION_SIZE = 4\n",
    "CONVOLUTION_FILTERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431226a",
   "metadata": {},
   "source": [
    "### (a) Implement backward function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981236b",
   "metadata": {},
   "source": [
    "Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74b000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for a single example.\n",
    "    The shape of the input is of size # num classes.\n",
    "\n",
    "    Important Note: You must be careful to avoid overflow for this function. Functions\n",
    "    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n",
    "    You will know that your function is overflow resistent when it can handle input like:\n",
    "    np.array([[10000, 10010, 10]]) without issues.\n",
    "\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array containing the softmax results of shape  number_of_classes\n",
    "    \"\"\"\n",
    "    x = x - np.max(x, axis=0)\n",
    "    exp = np.exp(x)\n",
    "    s = exp / np.sum(exp, axis=0)\n",
    "    return s\n",
    "\n",
    "def backward_softmax(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x.\n",
    "\n",
    "    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n",
    "\n",
    "    Args:\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "        grad_outputs: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n",
    "    \"\"\"\n",
    "    \n",
    "    # *** START CODE HERE ***\n",
    "    softmax_x = forward_softmax(x)\n",
    "    dot_product = np.dot(grad_outputs, softmax_x)\n",
    "    grad_x = softmax_x * (grad_outputs - dot_product)\n",
    "    return grad_x\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937cd61",
   "metadata": {},
   "source": [
    "ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a663db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu function for the input x.\n",
    "\n",
    "    Args:\n",
    "        x: A numpy float array\n",
    "\n",
    "    Returns:\n",
    "        A numpy float array containing the relu results\n",
    "    \"\"\"\n",
    "    \n",
    "    x[x<=0] = 0\n",
    "    return x\n",
    "\n",
    "def backward_relu(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x\n",
    "\n",
    "    Args:\n",
    "        x: A numpy array of arbitrary shape containing the input.\n",
    "        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n",
    "            to the output of relu\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of the same shape as x containing the gradients with respect to x.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    grad_x = grad_outputs * (x > 0).astype(float)\n",
    "    return grad_x\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005fb8fe",
   "metadata": {},
   "source": [
    "Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f30498c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the output from a cross entropy loss layer given the probabilities and labels.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be a scalar\n",
    "\n",
    "    Returns:\n",
    "        The result of the log loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    result = 0\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            result += -np.log(probabilities[i])\n",
    "\n",
    "    return result\n",
    "\n",
    "def backward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross entropy loss with respect to the probabilities.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be the gradient with respect to the probabilities.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    grad_prob = np.zeros_like(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            grad_prob[i] = -1 / probabilities[i]\n",
    "    return grad_prob\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe88264",
   "metadata": {},
   "source": [
    "Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a17be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear(weights, bias, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a linear layer with the given weights, bias and data.\n",
    "    weights is of the shape (input # features, output # features)\n",
    "    bias is of the shape (output # features)\n",
    "    data is of the shape (input # features)\n",
    "\n",
    "    The output should be of the shape (output # features)\n",
    "\n",
    "    Returns:\n",
    "        The result of the linear layer\n",
    "    \"\"\"\n",
    "    return data.dot(weights) + bias\n",
    "\n",
    "def backward_linear(weights, bias, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the loss with respect to the parameters of a linear layer.\n",
    "\n",
    "    See forward_linear for information about the shapes of the variables.\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of this layer.\n",
    "\n",
    "    This should return a tuple with three elements:\n",
    "    - The gradient of the loss with respect to the weights\n",
    "    - The gradient of the loss with respect to the bias\n",
    "    - The gradient of the loss with respect to the data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    grad_data = np.dot(output_grad, weights.T)\n",
    "    grad_weights = np.outer(data, output_grad)\n",
    "    grad_bias = output_grad\n",
    "    return grad_weights, grad_bias, grad_data\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9f793",
   "metadata": {},
   "source": [
    "2-D Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4baeb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = np.sum(\n",
    "                    np.multiply(data[:, x:(x + conv_width), y:(y + conv_height)], conv_W[output_channel, :, :, :])) + conv_b[output_channel]\n",
    "\n",
    "    return output\n",
    "\n",
    "def backward_convolution(conv_W, conv_b, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the parameters of the convolution.\n",
    "\n",
    "    See forward_convolution for the sizes of the arguments.\n",
    "    output_grad is the gradient of the loss with respect to the output of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing 3 gradients.\n",
    "        The first element is the gradient of the loss with respect to the convolution weights\n",
    "        The second element is the gradient of the loss with respect to the convolution bias\n",
    "        The third element is the gradient of the loss with respect to the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    output_width, output_height = input_width - conv_width + 1, input_height - conv_height + 1\n",
    "\n",
    "    grad_w = np.zeros_like(conv_W)\n",
    "    grad_data = np.zeros_like(data)\n",
    "    for x in range(output_width):\n",
    "        for y in range(output_height):\n",
    "            for output_channel in range(conv_channels):\n",
    "                grad_w[output_channel] += output_grad[output_channel, x, y] * data[:, x: x + conv_width, y: y + conv_height]\n",
    "                grad_data[:, x: x + conv_width, y: y + conv_height] += output_grad[output_channel, x, y] * conv_W[output_channel]\n",
    "    \n",
    "    grad_b = np.sum(output_grad, axis=(1, 2))\n",
    "\n",
    "    return grad_w, grad_b, grad_data\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76ac53",
   "metadata": {},
   "source": [
    "Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6e04c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_max_pool(data, pool_width, pool_height):\n",
    "    \"\"\"\n",
    "    Compute the output from a max pooling layer given the data and pool dimensions.\n",
    "\n",
    "    The stride length should be equal to the pool size\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "\n",
    "    The output should be the result of the max pooling layer and should be of size:\n",
    "        (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    Returns:\n",
    "        The result of the max pooling layer\n",
    "    \"\"\"\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    \n",
    "    output = np.zeros((input_channels, input_width // pool_width, input_height // pool_height))\n",
    "\n",
    "    for x in range(0, input_width, pool_width):\n",
    "        for y in range(0, input_height, pool_height):\n",
    "\n",
    "            output[:, x // pool_width, y // pool_height] = np.amax(data[:, x:(x + pool_width), y:(y + pool_height)], axis=(1, 2))\n",
    "\n",
    "    return output\n",
    "\n",
    "def backward_max_pool(data, pool_width, pool_height, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the data in the max pooling layer.\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of the backward max\n",
    "    pool layer.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the data (of same shape as data)\n",
    "    \"\"\"\n",
    "    \n",
    "    # *** START CODE HERE ***\n",
    "    num_channels, width, height = data.shape\n",
    "    output_width, output_height = width // pool_width, height // pool_height\n",
    "\n",
    "    grad_data = np.zeros_like(data)\n",
    "    for x in range(output_width):\n",
    "        for y in range(output_height):\n",
    "            for channel in range(num_channels):\n",
    "                start_x = x * pool_width\n",
    "                end_x = start_x + pool_width\n",
    "                start_y = y * pool_height\n",
    "                end_y = start_y + pool_height\n",
    "                pool_window = data[channel, start_x: end_x, start_y: end_y]\n",
    "                idx = np.unravel_index(np.argmax(pool_window), pool_window.shape)\n",
    "                grad_data[channel, idx] = grad_data[channel, x, y]\n",
    "    \n",
    "    return grad_data\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e3d90",
   "metadata": {},
   "source": [
    "### (b) Implement backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "121bfb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_params():\n",
    "    \"\"\"\n",
    "    Compute the initial parameters for the neural network.\n",
    "\n",
    "    This function should return a dictionary mapping parameter names to numpy arrays containing\n",
    "    the initial values for those parameters.\n",
    "\n",
    "    There should be four parameters for this model:\n",
    "    W1 is the weight matrix for the convolutional layer\n",
    "    b1 is the bias vector for the convolutional layer\n",
    "    W2 is the weight matrix for the output layers\n",
    "    b2 is the bias vector for the output layer\n",
    "\n",
    "    Weight matrices should be initialized with values drawn from a random normal distribution.\n",
    "    The mean of that distribution should be 0.\n",
    "    The variance of that distribution should be 1/sqrt(n) where n is the number of neurons that\n",
    "    feed into an output for that layer.\n",
    "\n",
    "    Bias vectors should be initialized with zero.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        A dict mapping parameter names to numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    size_after_convolution = 28 - CONVOLUTION_SIZE + 1\n",
    "    size_after_max_pooling = size_after_convolution // MAX_POOL_SIZE\n",
    "\n",
    "    num_hidden = size_after_max_pooling * size_after_max_pooling * CONVOLUTION_FILTERS\n",
    "\n",
    "    return {\n",
    "        'W1': np.random.normal(size = (CONVOLUTION_FILTERS, 1, CONVOLUTION_SIZE, CONVOLUTION_SIZE), scale=1/ math.sqrt(CONVOLUTION_SIZE * CONVOLUTION_SIZE)),\n",
    "        'b1': np.zeros(CONVOLUTION_FILTERS),\n",
    "        'W2': np.random.normal(size = (num_hidden, 10), scale = 1/ math.sqrt(num_hidden)),\n",
    "        'b2': np.zeros(10)\n",
    "    }\n",
    "\n",
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the forward layer given the data, labels, and params.\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input (shape is 1 by 28 by 28)\n",
    "        labels: A 1d numpy array containing the labels (shape is 10)\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A 2 element tuple containing:\n",
    "            1. A numpy array The output (after the softmax) of the output layer\n",
    "            2. The average loss for these data elements\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "    \n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "    cost = forward_cross_entropy_loss(y, labels)\n",
    "\n",
    "    return y, cost\n",
    "\n",
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation gradient computation step for a neural network\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input for a single example\n",
    "        labels: A 1d numpy array containing the labels for a single example\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2, and b2\n",
    "            W1 and b1 represent the weights and bias for the convolutional layer\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of strings to numpy arrays where each key represents the name of a weight\n",
    "        and the values represent the gradient of the loss with respect to that weight.\n",
    "        \n",
    "        In particular, it should have 4 elements:\n",
    "            W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "    \n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "\n",
    "    grad_y = backward_cross_entropy_loss(y, labels)\n",
    "    grad_logits = backward_softmax(logits, grad_y)\n",
    "    grad_W2, grad_b2, grad_flattened = backward_linear(W2, b2, flattened, grad_logits)\n",
    "    grad_first_after_relu = grad_flattened.reshape(first_after_relu.shape)\n",
    "    grad_first_max_pool = backward_relu(first_max_pool, grad_first_after_relu)\n",
    "    grad_first_convolution = backward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE, grad_first_max_pool)\n",
    "    grad_W1, grad_b1, grad_data = backward_convolution(W1, b1, data, grad_first_convolution)\n",
    "\n",
    "    return {'W1': grad_W1, 'W2': grad_W2, 'b1': grad_b1, 'b2': grad_b2}\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "def forward_prop_batch(batch_data, batch_labels, params, forward_prop_func):\n",
    "    \"\"\"Apply the forward prop func to every image in a batch\"\"\"\n",
    "\n",
    "    y_array = []\n",
    "    cost_array = []\n",
    "\n",
    "    for item, label in zip(batch_data, batch_labels):\n",
    "        y, cost = forward_prop_func(item, label, params)\n",
    "        y_array.append(y)\n",
    "        cost_array.append(cost)\n",
    "\n",
    "    return np.array(y_array), np.array(cost_array)\n",
    "\n",
    "def gradient_descent_batch(batch_data, batch_labels, learning_rate, params, backward_prop_func):\n",
    "    \"\"\"\n",
    "    Perform one batch of gradient descent on the given training data using the provided learning rate.\n",
    "\n",
    "    This code should update the parameters stored in params.\n",
    "    It should not return anything\n",
    "\n",
    "    Args:\n",
    "        batch_data: A numpy array containing the training data for the batch\n",
    "        train_labels: A numpy array containing the training labels for the batch\n",
    "        learning_rate: The learning rate\n",
    "        params: A dict of parameter names to parameter values that should be updated.\n",
    "        backward_prop_func: A function that follows the backwards_prop API\n",
    "\n",
    "    Returns: This function returns nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    total_grad = {}\n",
    "\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        grad = backward_prop_func(\n",
    "            batch_data[i, :, :],\n",
    "            batch_labels[i, :],\n",
    "            params)\n",
    "        for key, value in grad.items():\n",
    "            if key not in total_grad:\n",
    "                total_grad[key] = np.zeros(value.shape)\n",
    "\n",
    "            total_grad[key] += value\n",
    "\n",
    "    params['W1'] = params['W1'] - learning_rate * total_grad['W1']\n",
    "    params['W2'] = params['W2'] - learning_rate * total_grad['W2']\n",
    "    params['b1'] = params['b1'] - learning_rate * total_grad['b1']\n",
    "    params['b2'] = params['b2'] - learning_rate * total_grad['b2']\n",
    "\n",
    "    # This function does not return anything\n",
    "    return\n",
    "\n",
    "def nn_train(\n",
    "    train_data, train_labels, dev_data, dev_labels,\n",
    "    get_initial_params_func, forward_prop_func, backward_prop_func,\n",
    "    learning_rate=5.0, batch_size=16, num_batches=400):\n",
    "\n",
    "    m = train_data.shape[0]\n",
    "\n",
    "    params = get_initial_params_func()\n",
    "\n",
    "    cost_dev = []\n",
    "    accuracy_dev = []\n",
    "    for batch in range(num_batches):\n",
    "        print('Currently processing {} / {}'.format(batch, num_batches))\n",
    "\n",
    "        batch_data = train_data[batch * batch_size:(batch + 1) * batch_size, :, :, :]\n",
    "        batch_labels = train_labels[batch * batch_size: (batch + 1) * batch_size, :]\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            output, cost = forward_prop_batch(dev_data, dev_labels, params, forward_prop_func)\n",
    "            cost_dev.append(sum(cost) / len(cost))\n",
    "            accuracy_dev.append(compute_accuracy(output, dev_labels))\n",
    "\n",
    "            print('Cost and accuracy', cost_dev[-1], accuracy_dev[-1])\n",
    "\n",
    "        gradient_descent_batch(batch_data, batch_labels,\n",
    "            learning_rate, params, backward_prop_func)\n",
    "\n",
    "    return params, cost_dev, accuracy_dev\n",
    "\n",
    "def nn_test(data, labels, params):\n",
    "    output, cost = forward_prop(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    correct_output = np.argmax(output,axis=1)\n",
    "    correct_labels = np.argmax(labels,axis=1)\n",
    "\n",
    "    is_correct = [a == b for a,b in zip(correct_output, correct_labels)]\n",
    "\n",
    "    accuracy = sum(is_correct) * 1. / labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def read_data(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "\n",
    "    x = np.reshape(x, (x.shape[0], 1, 28, 28))\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def run_train(all_data, all_labels, backward_prop_func):\n",
    "    params, cost_dev, accuracy_dev = nn_train(\n",
    "        all_data['train'], all_labels['train'],\n",
    "        all_data['dev'], all_labels['dev'],\n",
    "        get_initial_params, forward_prop, backward_prop_func,\n",
    "        learning_rate=1e-2, batch_size=16, num_batches=400\n",
    "    )\n",
    "\n",
    "    t = np.arange(400 // 100)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    ax1.plot(t, cost_dev, 'b')\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_title('Training curve')\n",
    "\n",
    "    ax2.plot(t, accuracy_dev, 'b')\n",
    "    ax2.set_xlabel('time')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "\n",
    "    fig.savefig('output/train.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c3d09d",
   "metadata": {},
   "source": [
    "### Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ba386e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing 0 / 400\n",
      "Cost and accuracy 2.721417647426753 0.0725\n",
      "Currently processing 1 / 400\n",
      "Currently processing 2 / 400\n",
      "Currently processing 3 / 400\n",
      "Currently processing 4 / 400\n",
      "Currently processing 5 / 400\n",
      "Currently processing 6 / 400\n",
      "Currently processing 7 / 400\n",
      "Currently processing 8 / 400\n",
      "Currently processing 9 / 400\n",
      "Currently processing 10 / 400\n",
      "Currently processing 11 / 400\n",
      "Currently processing 12 / 400\n",
      "Currently processing 13 / 400\n",
      "Currently processing 14 / 400\n",
      "Currently processing 15 / 400\n",
      "Currently processing 16 / 400\n",
      "Currently processing 17 / 400\n",
      "Currently processing 18 / 400\n",
      "Currently processing 19 / 400\n",
      "Currently processing 20 / 400\n",
      "Currently processing 21 / 400\n",
      "Currently processing 22 / 400\n",
      "Currently processing 23 / 400\n",
      "Currently processing 24 / 400\n",
      "Currently processing 25 / 400\n",
      "Currently processing 26 / 400\n",
      "Currently processing 27 / 400\n",
      "Currently processing 28 / 400\n",
      "Currently processing 29 / 400\n",
      "Currently processing 30 / 400\n",
      "Currently processing 31 / 400\n",
      "Currently processing 32 / 400\n",
      "Currently processing 33 / 400\n",
      "Currently processing 34 / 400\n",
      "Currently processing 35 / 400\n",
      "Currently processing 36 / 400\n",
      "Currently processing 37 / 400\n",
      "Currently processing 38 / 400\n",
      "Currently processing 39 / 400\n",
      "Currently processing 40 / 400\n",
      "Currently processing 41 / 400\n",
      "Currently processing 42 / 400\n",
      "Currently processing 43 / 400\n",
      "Currently processing 44 / 400\n",
      "Currently processing 45 / 400\n",
      "Currently processing 46 / 400\n",
      "Currently processing 47 / 400\n",
      "Currently processing 48 / 400\n",
      "Currently processing 49 / 400\n",
      "Currently processing 50 / 400\n",
      "Currently processing 51 / 400\n",
      "Currently processing 52 / 400\n",
      "Currently processing 53 / 400\n",
      "Currently processing 54 / 400\n",
      "Currently processing 55 / 400\n",
      "Currently processing 56 / 400\n",
      "Currently processing 57 / 400\n",
      "Currently processing 58 / 400\n",
      "Currently processing 59 / 400\n",
      "Currently processing 60 / 400\n",
      "Currently processing 61 / 400\n",
      "Currently processing 62 / 400\n",
      "Currently processing 63 / 400\n",
      "Currently processing 64 / 400\n",
      "Currently processing 65 / 400\n",
      "Currently processing 66 / 400\n",
      "Currently processing 67 / 400\n",
      "Currently processing 68 / 400\n",
      "Currently processing 69 / 400\n",
      "Currently processing 70 / 400\n",
      "Currently processing 71 / 400\n",
      "Currently processing 72 / 400\n",
      "Currently processing 73 / 400\n",
      "Currently processing 74 / 400\n",
      "Currently processing 75 / 400\n",
      "Currently processing 76 / 400\n",
      "Currently processing 77 / 400\n",
      "Currently processing 78 / 400\n",
      "Currently processing 79 / 400\n",
      "Currently processing 80 / 400\n",
      "Currently processing 81 / 400\n",
      "Currently processing 82 / 400\n",
      "Currently processing 83 / 400\n",
      "Currently processing 84 / 400\n",
      "Currently processing 85 / 400\n",
      "Currently processing 86 / 400\n",
      "Currently processing 87 / 400\n",
      "Currently processing 88 / 400\n",
      "Currently processing 89 / 400\n",
      "Currently processing 90 / 400\n",
      "Currently processing 91 / 400\n",
      "Currently processing 92 / 400\n",
      "Currently processing 93 / 400\n",
      "Currently processing 94 / 400\n",
      "Currently processing 95 / 400\n",
      "Currently processing 96 / 400\n",
      "Currently processing 97 / 400\n",
      "Currently processing 98 / 400\n",
      "Currently processing 99 / 400\n",
      "Currently processing 100 / 400\n",
      "Cost and accuracy 0.909511913412193 0.685\n",
      "Currently processing 101 / 400\n",
      "Currently processing 102 / 400\n",
      "Currently processing 103 / 400\n",
      "Currently processing 104 / 400\n",
      "Currently processing 105 / 400\n",
      "Currently processing 106 / 400\n",
      "Currently processing 107 / 400\n",
      "Currently processing 108 / 400\n",
      "Currently processing 109 / 400\n",
      "Currently processing 110 / 400\n",
      "Currently processing 111 / 400\n",
      "Currently processing 112 / 400\n",
      "Currently processing 113 / 400\n",
      "Currently processing 114 / 400\n",
      "Currently processing 115 / 400\n",
      "Currently processing 116 / 400\n",
      "Currently processing 117 / 400\n",
      "Currently processing 118 / 400\n",
      "Currently processing 119 / 400\n",
      "Currently processing 120 / 400\n",
      "Currently processing 121 / 400\n",
      "Currently processing 122 / 400\n",
      "Currently processing 123 / 400\n",
      "Currently processing 124 / 400\n",
      "Currently processing 125 / 400\n",
      "Currently processing 126 / 400\n",
      "Currently processing 127 / 400\n",
      "Currently processing 128 / 400\n",
      "Currently processing 129 / 400\n",
      "Currently processing 130 / 400\n",
      "Currently processing 131 / 400\n",
      "Currently processing 132 / 400\n",
      "Currently processing 133 / 400\n",
      "Currently processing 134 / 400\n",
      "Currently processing 135 / 400\n",
      "Currently processing 136 / 400\n",
      "Currently processing 137 / 400\n",
      "Currently processing 138 / 400\n",
      "Currently processing 139 / 400\n",
      "Currently processing 140 / 400\n",
      "Currently processing 141 / 400\n",
      "Currently processing 142 / 400\n",
      "Currently processing 143 / 400\n",
      "Currently processing 144 / 400\n",
      "Currently processing 145 / 400\n",
      "Currently processing 146 / 400\n",
      "Currently processing 147 / 400\n",
      "Currently processing 148 / 400\n",
      "Currently processing 149 / 400\n",
      "Currently processing 150 / 400\n",
      "Currently processing 151 / 400\n",
      "Currently processing 152 / 400\n",
      "Currently processing 153 / 400\n",
      "Currently processing 154 / 400\n",
      "Currently processing 155 / 400\n",
      "Currently processing 156 / 400\n",
      "Currently processing 157 / 400\n",
      "Currently processing 158 / 400\n",
      "Currently processing 159 / 400\n",
      "Currently processing 160 / 400\n",
      "Currently processing 161 / 400\n",
      "Currently processing 162 / 400\n",
      "Currently processing 163 / 400\n",
      "Currently processing 164 / 400\n",
      "Currently processing 165 / 400\n",
      "Currently processing 166 / 400\n",
      "Currently processing 167 / 400\n",
      "Currently processing 168 / 400\n",
      "Currently processing 169 / 400\n",
      "Currently processing 170 / 400\n",
      "Currently processing 171 / 400\n",
      "Currently processing 172 / 400\n",
      "Currently processing 173 / 400\n",
      "Currently processing 174 / 400\n",
      "Currently processing 175 / 400\n",
      "Currently processing 176 / 400\n",
      "Currently processing 177 / 400\n",
      "Currently processing 178 / 400\n",
      "Currently processing 179 / 400\n",
      "Currently processing 180 / 400\n",
      "Currently processing 181 / 400\n",
      "Currently processing 182 / 400\n",
      "Currently processing 183 / 400\n",
      "Currently processing 184 / 400\n",
      "Currently processing 185 / 400\n",
      "Currently processing 186 / 400\n",
      "Currently processing 187 / 400\n",
      "Currently processing 188 / 400\n",
      "Currently processing 189 / 400\n",
      "Currently processing 190 / 400\n",
      "Currently processing 191 / 400\n",
      "Currently processing 192 / 400\n",
      "Currently processing 193 / 400\n",
      "Currently processing 194 / 400\n",
      "Currently processing 195 / 400\n",
      "Currently processing 196 / 400\n",
      "Currently processing 197 / 400\n",
      "Currently processing 198 / 400\n",
      "Currently processing 199 / 400\n",
      "Currently processing 200 / 400\n",
      "Cost and accuracy 0.6824539073859571 0.7775\n",
      "Currently processing 201 / 400\n",
      "Currently processing 202 / 400\n",
      "Currently processing 203 / 400\n",
      "Currently processing 204 / 400\n",
      "Currently processing 205 / 400\n",
      "Currently processing 206 / 400\n",
      "Currently processing 207 / 400\n",
      "Currently processing 208 / 400\n",
      "Currently processing 209 / 400\n",
      "Currently processing 210 / 400\n",
      "Currently processing 211 / 400\n",
      "Currently processing 212 / 400\n",
      "Currently processing 213 / 400\n",
      "Currently processing 214 / 400\n",
      "Currently processing 215 / 400\n",
      "Currently processing 216 / 400\n",
      "Currently processing 217 / 400\n",
      "Currently processing 218 / 400\n",
      "Currently processing 219 / 400\n",
      "Currently processing 220 / 400\n",
      "Currently processing 221 / 400\n",
      "Currently processing 222 / 400\n",
      "Currently processing 223 / 400\n",
      "Currently processing 224 / 400\n",
      "Currently processing 225 / 400\n",
      "Currently processing 226 / 400\n",
      "Currently processing 227 / 400\n",
      "Currently processing 228 / 400\n",
      "Currently processing 229 / 400\n",
      "Currently processing 230 / 400\n",
      "Currently processing 231 / 400\n",
      "Currently processing 232 / 400\n",
      "Currently processing 233 / 400\n",
      "Currently processing 234 / 400\n",
      "Currently processing 235 / 400\n",
      "Currently processing 236 / 400\n",
      "Currently processing 237 / 400\n",
      "Currently processing 238 / 400\n",
      "Currently processing 239 / 400\n",
      "Currently processing 240 / 400\n",
      "Currently processing 241 / 400\n",
      "Currently processing 242 / 400\n",
      "Currently processing 243 / 400\n",
      "Currently processing 244 / 400\n",
      "Currently processing 245 / 400\n",
      "Currently processing 246 / 400\n",
      "Currently processing 247 / 400\n",
      "Currently processing 248 / 400\n",
      "Currently processing 249 / 400\n",
      "Currently processing 250 / 400\n",
      "Currently processing 251 / 400\n",
      "Currently processing 252 / 400\n",
      "Currently processing 253 / 400\n",
      "Currently processing 254 / 400\n",
      "Currently processing 255 / 400\n",
      "Currently processing 256 / 400\n",
      "Currently processing 257 / 400\n",
      "Currently processing 258 / 400\n",
      "Currently processing 259 / 400\n",
      "Currently processing 260 / 400\n",
      "Currently processing 261 / 400\n",
      "Currently processing 262 / 400\n",
      "Currently processing 263 / 400\n",
      "Currently processing 264 / 400\n",
      "Currently processing 265 / 400\n",
      "Currently processing 266 / 400\n",
      "Currently processing 267 / 400\n",
      "Currently processing 268 / 400\n",
      "Currently processing 269 / 400\n",
      "Currently processing 270 / 400\n",
      "Currently processing 271 / 400\n",
      "Currently processing 272 / 400\n",
      "Currently processing 273 / 400\n",
      "Currently processing 274 / 400\n",
      "Currently processing 275 / 400\n",
      "Currently processing 276 / 400\n",
      "Currently processing 277 / 400\n",
      "Currently processing 278 / 400\n",
      "Currently processing 279 / 400\n",
      "Currently processing 280 / 400\n",
      "Currently processing 281 / 400\n",
      "Currently processing 282 / 400\n",
      "Currently processing 283 / 400\n",
      "Currently processing 284 / 400\n",
      "Currently processing 285 / 400\n",
      "Currently processing 286 / 400\n",
      "Currently processing 287 / 400\n",
      "Currently processing 288 / 400\n",
      "Currently processing 289 / 400\n",
      "Currently processing 290 / 400\n",
      "Currently processing 291 / 400\n",
      "Currently processing 292 / 400\n",
      "Currently processing 293 / 400\n",
      "Currently processing 294 / 400\n",
      "Currently processing 295 / 400\n",
      "Currently processing 296 / 400\n",
      "Currently processing 297 / 400\n",
      "Currently processing 298 / 400\n",
      "Currently processing 299 / 400\n",
      "Currently processing 300 / 400\n",
      "Cost and accuracy 0.6765130557165203 0.7825\n",
      "Currently processing 301 / 400\n",
      "Currently processing 302 / 400\n",
      "Currently processing 303 / 400\n",
      "Currently processing 304 / 400\n",
      "Currently processing 305 / 400\n",
      "Currently processing 306 / 400\n",
      "Currently processing 307 / 400\n",
      "Currently processing 308 / 400\n",
      "Currently processing 309 / 400\n",
      "Currently processing 310 / 400\n",
      "Currently processing 311 / 400\n",
      "Currently processing 312 / 400\n",
      "Currently processing 313 / 400\n",
      "Currently processing 314 / 400\n",
      "Currently processing 315 / 400\n",
      "Currently processing 316 / 400\n",
      "Currently processing 317 / 400\n",
      "Currently processing 318 / 400\n",
      "Currently processing 319 / 400\n",
      "Currently processing 320 / 400\n",
      "Currently processing 321 / 400\n",
      "Currently processing 322 / 400\n",
      "Currently processing 323 / 400\n",
      "Currently processing 324 / 400\n",
      "Currently processing 325 / 400\n",
      "Currently processing 326 / 400\n",
      "Currently processing 327 / 400\n",
      "Currently processing 328 / 400\n",
      "Currently processing 329 / 400\n",
      "Currently processing 330 / 400\n",
      "Currently processing 331 / 400\n",
      "Currently processing 332 / 400\n",
      "Currently processing 333 / 400\n",
      "Currently processing 334 / 400\n",
      "Currently processing 335 / 400\n",
      "Currently processing 336 / 400\n",
      "Currently processing 337 / 400\n",
      "Currently processing 338 / 400\n",
      "Currently processing 339 / 400\n",
      "Currently processing 340 / 400\n",
      "Currently processing 341 / 400\n",
      "Currently processing 342 / 400\n",
      "Currently processing 343 / 400\n",
      "Currently processing 344 / 400\n",
      "Currently processing 345 / 400\n",
      "Currently processing 346 / 400\n",
      "Currently processing 347 / 400\n",
      "Currently processing 348 / 400\n",
      "Currently processing 349 / 400\n",
      "Currently processing 350 / 400\n",
      "Currently processing 351 / 400\n",
      "Currently processing 352 / 400\n",
      "Currently processing 353 / 400\n",
      "Currently processing 354 / 400\n",
      "Currently processing 355 / 400\n",
      "Currently processing 356 / 400\n",
      "Currently processing 357 / 400\n",
      "Currently processing 358 / 400\n",
      "Currently processing 359 / 400\n",
      "Currently processing 360 / 400\n",
      "Currently processing 361 / 400\n",
      "Currently processing 362 / 400\n",
      "Currently processing 363 / 400\n",
      "Currently processing 364 / 400\n",
      "Currently processing 365 / 400\n",
      "Currently processing 366 / 400\n",
      "Currently processing 367 / 400\n",
      "Currently processing 368 / 400\n",
      "Currently processing 369 / 400\n",
      "Currently processing 370 / 400\n",
      "Currently processing 371 / 400\n",
      "Currently processing 372 / 400\n",
      "Currently processing 373 / 400\n",
      "Currently processing 374 / 400\n",
      "Currently processing 375 / 400\n",
      "Currently processing 376 / 400\n",
      "Currently processing 377 / 400\n",
      "Currently processing 378 / 400\n",
      "Currently processing 379 / 400\n",
      "Currently processing 380 / 400\n",
      "Currently processing 381 / 400\n",
      "Currently processing 382 / 400\n",
      "Currently processing 383 / 400\n",
      "Currently processing 384 / 400\n",
      "Currently processing 385 / 400\n",
      "Currently processing 386 / 400\n",
      "Currently processing 387 / 400\n",
      "Currently processing 388 / 400\n",
      "Currently processing 389 / 400\n",
      "Currently processing 390 / 400\n",
      "Currently processing 391 / 400\n",
      "Currently processing 392 / 400\n",
      "Currently processing 393 / 400\n",
      "Currently processing 394 / 400\n",
      "Currently processing 395 / 400\n",
      "Currently processing 396 / 400\n",
      "Currently processing 397 / 400\n",
      "Currently processing 398 / 400\n",
      "Currently processing 399 / 400\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "train_data, train_labels = read_data('./data/images_train.csv', './data/labels_train.csv')\n",
    "train_labels = one_hot_labels(train_labels)\n",
    "p = np.random.permutation(60000)\n",
    "train_data = train_data[p,:]\n",
    "train_labels = train_labels[p,:]\n",
    "\n",
    "dev_data = train_data[0:400,:]\n",
    "dev_labels = train_labels[0:400,:]\n",
    "train_data = train_data[400:,:]\n",
    "train_labels = train_labels[400:,:]\n",
    "\n",
    "mean = np.mean(train_data)\n",
    "std = np.std(train_data)\n",
    "train_data = (train_data - mean) / std\n",
    "dev_data = (dev_data - mean) / std\n",
    "\n",
    "all_data = {\n",
    "    'train': train_data,\n",
    "    'dev': dev_data,\n",
    "}\n",
    "\n",
    "all_labels = {\n",
    "    'train': train_labels,\n",
    "    'dev': dev_labels,\n",
    "}\n",
    "    \n",
    "run_train(all_data, all_labels, backward_prop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_snn_ei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
